# type: ignore

import torch
import datasets
from tqdm import tqdm
import numpy as np
from torch.nn import functional as F
from typing import cast

# Import your custom classes
from lib_chunked_diffusion_model_config import ChunkedDiffusionModelConfig
from lib_chunked_diffusion_model import ChunkedDiffusionSystem

# A placeholder for safe code execution.
# In a real-world scenario, this should use a sandboxed environment
# like a Docker container or a restricted execution library to prevent security risks.
def execute_and_verify(code_to_execute: str, test_cases: str) -> bool:
    """
    Executes the generated code against provided test cases in a safe manner.

    Args:
        code_to_execute (str): The Python code generated by the model.
        test_cases (str): The unit tests to validate the code.

    Returns:
        bool: True if all tests pass, False otherwise.
    """
    try:
        # Combine the generated code and the tests
        full_code = code_to_execute + "\n" + test_cases

        # Create a dictionary to hold the execution scope
        exec_scope = {}

        # Execute the code. This is UNSAFE and for demonstration only.
        # Use a proper sandbox for production.
        exec(full_code, exec_scope)

        return True # If exec completes without an AssertionError, we assume it passed.
    except Exception as e:
        # This will catch syntax errors, assertion errors from tests, etc.
        # print(f"Code execution failed: {e}")
        return False


class Evaluator:
    """
    A comprehensive evaluator for the ChunkedDiffusionSystem.
    """
    def __init__(self, from_qlora_model: bool = False, batch_size: int = 1):
        print("Initializing the Chunked Diffusion System...")
        self.model_config = ChunkedDiffusionModelConfig(
            from_model_name="Qwen/Qwen2.5-0.5B",
            from_model_family="Qwen2",
            # Set to True if you are loading a QLoRA-quantized model
            from_qlora_model=from_qlora_model
        )
        self.cdllm = ChunkedDiffusionSystem(model_config=self.model_config)
        self.device = self.cdllm.device
        print(f"Model initialized on device: {self.device}")

        # Disable gradients for all model parameters during evaluation
        for param in self.cdllm.parameters():
            param.requires_grad = False
        self.cdllm.model.eval()

    def generate_response(self, text: str, documents: dict[str, str] = None, max_length: int = 128) -> str:
        """
        Generates a response using the model's fast generation capability.

        Args:
            text (str): The input prompt or text.
            documents (dict[str, str], optional): Context documents. Defaults to None.
            max_length (int, optional): Maximum tokens to generate. Defaults to 128.

        Returns:
            str: The generated text.
        """
        return self.cdllm.next_token_prediction(
            text=text,
            documents=documents,
            max_length=max_length,
            # This is the key for fast generation, as analyzed from your code
            generate_n_toks_per_n_toks=16
        )

    def calculate_sequence_loss(self, text: str) -> float:
        """
        Calculates the cross-entropy loss for an entire text sequence.
        This is used to score multiple-choice options.
        Adapted from the loss logic in your Trainer class.
        """
        # Prepare chunks and context. This logic is derived from your `train_or_get_loss_on_one_text`
        chunks_docs, chunks_idx, chunks, chunks_lengths = self.cdllm.prepare_chunks_for_next_tokens_predictions(text=text)

        if not chunks:
            return float('inf')

        # We want to calculate the loss over all tokens in the main context
        main_doc_idx = len(chunks_docs) - 1

        all_logits = []
        all_truth_tokens = []

        # Iterate over each chunk of the main document
        for chunk_idx, chunk in enumerate(chunks):
            if chunks_idx[chunk_idx] != main_doc_idx:
                continue

            # Prepare context for this specific chunk
            context, permissions_mask, attention_causal_mask, _, embeddings_override = self.cdllm.prepare_context_and_masks_for_all_chunks(
                chunks_documents=chunks_docs,
                chunks_documents_idx=chunks_idx,
                chunks=chunks,
                current_chunk=chunk_idx
            )

            # Forward pass to get logits for the entire context
            logits, _ = self.cdllm.model.forward(
                input_ids=context,
                permissions_mask=permissions_mask,
                attention_causal_mask=attention_causal_mask,
                embedding_overide=embeddings_override
            )

            # Logits are for predicting the *next* token, so we shift
            # Logits for position i predict token at position i+1
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = context[..., 1:].contiguous()

            all_logits.append(shift_logits)
            all_truth_tokens.append(shift_labels)

        if not all_logits:
            return float('inf')

        # Concatenate results from all chunks
        final_logits = torch.cat([l.view(-1, l.size(-1)) for l in all_logits], dim=0)
        final_labels = torch.cat([t.view(-1) for t in all_truth_tokens], dim=0)

        # Calculate cross-entropy loss
        loss = F.cross_entropy(final_logits, final_labels)
        return loss.item()

    def evaluate_hellaswag(self, num_samples: int = 100):
        print("\n--- Starting HellaSwag Evaluation (Commonsense Reasoning) ---")
        dataset = datasets.load_dataset("Rowan/hellaswag", split="validation")
        samples = dataset.select(range(num_samples))

        correct = 0
        pbar = tqdm(samples, desc="Evaluating HellaSwag")
        for item in pbar:
            context = item['ctx']
            endings = item['endings']
            correct_label = int(item['label'])

            losses = []
            for ending in endings:
                full_sequence = context + " " + ending
                loss = self.calculate_sequence_loss(full_sequence)
                losses.append(loss)

            prediction = np.argmin(losses)
            if prediction == correct_label:
                correct += 1

            accuracy = (correct / max(1, pbar.n)) * 100
            pbar.set_postfix_str(f"Accuracy: {accuracy:.2f}%")

        final_accuracy = (correct / num_samples) * 100
        print(f"HellaSwag Final Accuracy: {final_accuracy:.2f}%")

    def evaluate_mmlu(self, num_samples: int = 50):
        print("\n--- Starting MMLU Evaluation (Multitask Knowledge) ---")
        # Using a single, small subject for a quick evaluation
        dataset = datasets.load_dataset("cais/mmlu", "high_school_computer_science", split="test")
        samples = dataset.select(range(num_samples))

        correct = 0
        pbar = tqdm(samples, desc="Evaluating MMLU")
        for item in pbar:
            question = item['question']
            choices = item['choices']
            correct_label = item['answer'] # This is an index

            prompt = f"Question: {question}\nChoices:\n"
            for i, choice in enumerate(choices):
                prompt += f"{chr(65+i)}. {choice}\n"
            prompt += "Answer:"

            losses = []
            for i, choice in enumerate(choices):
                # We calculate the loss for the prompt + the single character of the choice, e.g., "A"
                # This is a standard way to evaluate MCQs with generative models
                full_sequence = f"{prompt} {chr(65+i)}"
                loss = self.calculate_sequence_loss(full_sequence)
                losses.append(loss)

            prediction = np.argmin(losses)
            if prediction == correct_label:
                correct += 1

            accuracy = (correct / max(1, pbar.n)) * 100
            pbar.set_postfix_str(f"Accuracy: {accuracy:.2f}%")

        final_accuracy = (correct / num_samples) * 100
        print(f"MMLU (CS) Final Accuracy: {final_accuracy:.2f}%")

    def evaluate_humaneval(self, num_samples: int = 20):
        print("\n--- Starting HumanEval Evaluation (Code Generation) ---")
        dataset = datasets.load_dataset("openai_humaneval", split="test")
        samples = dataset.select(range(num_samples))

        passed = 0
        pbar = tqdm(samples, desc="Evaluating HumanEval")
        for item in pbar:
            prompt = item['prompt']
            test_cases = item['test']
            entry_point = item['entry_point']

            # The generated code should be the completion of the function
            generated_code = self.generate_response(text=prompt, max_length=256)

            # We need to construct the full function to test it
            full_code_to_test = prompt + generated_code

            # The test cases often need the function name to be defined
            test_cases_with_entrypoint = f"\ncheck({entry_point})\n"

            # Safely execute and verify
            if execute_and_verify(full_code_to_test, test_cases + test_cases_with_entrypoint):
                passed += 1

            pass_rate = (passed / max(1, pbar.n)) * 100
            pbar.set_postfix_str(f"Pass@1: {pass_rate:.2f}%")

        final_pass_rate = (passed / num_samples) * 100
        print(f"HumanEval Final Pass@1: {final_pass_rate:.2f}%")


if __name__ == "__main__":
    # Important: Set `from_qlora_model` to True if your model weights are 4-bit quantized.
    # Otherwise, set it to False.
    evaluator = Evaluator(from_qlora_model=False)

    with torch.no_grad():
        # Run evaluations
        evaluator.evaluate_hellaswag(num_samples=100)
        evaluator.evaluate_mmlu(num_samples=50)
        evaluator.evaluate_humaneval(num_samples=20)

    print("\nEvaluation complete.")